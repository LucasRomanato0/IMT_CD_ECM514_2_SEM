# -*- coding: utf-8 -*-
"""TRABALHO SEMESTRAL CD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n8g9qr7nVk59EWr3Dgzx6jswWe6bngqD

### Scraping
"""

def importxml(page, xpath):
    tree = html.fromstring(page.content)
    content_list = tree.xpath(xpath)
    content = content_list[0].text_content() if len(content_list) > 0 else ''
    default_encoding = sys.getdefaultencoding()
    print(f"content.strip(): {content.strip()}")
    return content.strip()

"""### Retirada das informações da FINEP"""

import requests
from lxml import html
import pandas as pd

# Função para extrair o conteúdo com base no XPath
def importxml(page, xpath):
    tree = html.fromstring(page.content)
    content_list = tree.xpath(xpath)
    content = content_list[0].text_content() if len(content_list) > 0 else ''
    return content.strip()

# Função para extrair todos os links de PDF de um edital
def extract_pdfs_from_edital(url, edital_number):
    page = requests.get(url)

    # XPath para encontrar todas as linhas que possuem documentos
    xpath_base = f'//*[@id="documentos-item-{edital_number}"]/tbody/tr/td[3]/a/@href'

    # Extrair todos os links para PDFs
    pdf_links = html.fromstring(page.content).xpath(xpath_base)

    return pdf_links

# Lista com os números de editais que você deseja percorrer
editais_numbers = list(range(586, 748))  # Exemplo de 586 até 747

# Lista para armazenar as informações
data = []

# Loop por cada edital
for edital_number in editais_numbers:
    url = f'http://finep.gov.br/chamadas-publicas/chamadapublica/{edital_number}'
    pdf_links = extract_pdfs_from_edital(url, edital_number)

    # Adiciona cada PDF do edital à lista de dados
    for pdf_link in pdf_links:
        data.append({'edital_number': edital_number, 'pdf_link': pdf_link})

# Convertendo a lista em dataframe
df = pd.DataFrame(data)

# Salvar os dados em um arquivo CSV
# df.to_csv('editais_finep.csv', index=False)

print("Extração concluída e dados salvos em 'editais_finep.csv'")

df

def fix_link(link):
    if link.startswith('/'):
        return "http://finep.gov.br" + link
    return link

df["link"] = df["pdf_link"].apply(fix_link)
df

df.to_csv('editais_finep.csv', index=False)

"""### Extração dos dados dos pdfs"""

!pip install pdfplumber
!pip install pypdf2

import requests
import pandas as pd
import os
import pdfplumber
from PyPDF2 import PdfReader
from requests.exceptions import ConnectionError, Timeout, RequestException

# Função para baixar o PDF a partir de um link
# def download_pdf(pdf_url, output_path):
#     response = requests.get(pdf_url)
#     with open(output_path, 'wb') as file:
#         file.write(response.content)

def download_pdf(pdf_url, output_path):
    try:
        response = requests.get(pdf_url)
        response.raise_for_status()  # Verifica se há algum erro HTTP
        with open(output_path, 'wb') as file:
            file.write(response.content)
        print(f"PDF baixado com sucesso: {pdf_url}")
    except ConnectionError:
        print(f"Erro de conexão ao tentar acessar {pdf_url}")
    except Timeout:
        print(f"O pedido para {pdf_url} expirou")
    except RequestException as e:
        print(f"Um erro ocorreu ao tentar baixar {pdf_url}: {e}")

# Função para extrair texto de um PDF
# def extract_text_from_pdf(pdf_path):
#     text = ''
#     with pdfplumber.open(pdf_path) as pdf:
#         for page in pdf.pages:
#             text += page.extract_text() + '\n'
#     return text

def extract_text_from_pdf(pdf_path):
    text = ''
    with open(pdf_path, 'rb') as file:
        reader = PdfReader(file)
        for page in reader.pages:
            text += page.extract_text() + '\n'
    return text

# Carregar o dataframe com os links dos PDFs
df = pd.read_csv('editais_finep.csv')

# Diretório para salvar os PDFs baixados
pdf_dir = 'pdfs_finep'
os.makedirs(pdf_dir, exist_ok=True)

# Lista para armazenar os textos extraídos dos PDFs
pdf_texts = []

# Loop sobre os links no dataframe
for index, row in df.iterrows():
    pdf_link = row['link']
    edital_number = row['edital_number']

    # Nome do arquivo PDF a ser salvo
    pdf_filename = f'edital_{edital_number}_{index}.pdf'
    pdf_path = os.path.join(pdf_dir, pdf_filename)

    # Baixar o PDF
    print(f"Baixando {pdf_link}...")
    download_pdf(pdf_link, pdf_path)

    # Extrair o texto do PDF
    print(f"Extraindo texto de {pdf_filename}...")
    pdf_text = extract_text_from_pdf(pdf_path)

    # Armazenar no dataframe
    pdf_texts.append({'edital_number': edital_number, 'pdf_link': pdf_link, 'pdf_text': pdf_text})

# Converter a lista em um dataframe
df_texts = pd.DataFrame(pdf_texts)

# Salvar o dataframe com o texto dos PDFs
df_texts.to_csv('editais_finep_textos.csv', index=False)

print("Extração de textos concluída e dados salvos em 'editais_finep_textos.csv'")

df

import requests
import pandas as pd
import os
from PyPDF2 import PdfReader
from requests.exceptions import ConnectionError, Timeout, RequestException

# Função para baixar o PDF a partir de um link
def download_pdf(pdf_url, output_path):
    try:
        response = requests.get(pdf_url)
        response.raise_for_status()  # Verifica se há algum erro HTTP
        with open(output_path, 'wb') as file:
            file.write(response.content)
        print(f"PDF baixado com sucesso: {pdf_url}")
        return True  # Retorna True se o download foi bem-sucedido
    except ConnectionError:
        print(f"Erro de conexão ao tentar acessar {pdf_url}")
    except Timeout:
        print(f"O pedido para {pdf_url} expirou")
    except RequestException as e:
        print(f"Um erro ocorreu ao tentar baixar {pdf_url}: {e}")
    return False  # Retorna False se o download falhou

# Função para extrair texto de um PDF
def extract_text_from_pdf(pdf_path):
    text = ''
    try:
        with open(pdf_path, 'rb') as file:
            reader = PdfReader(file)
            for page in reader.pages:
                text += page.extract_text() + '\n'
    except Exception as e:
        print(f"Erro ao extrair texto de {pdf_path}: {e}")
    return text

# Carregar o dataframe com os links dos PDFs
df = pd.read_csv('editais_finep.csv')

# Diretório para salvar os PDFs baixados
pdf_dir = 'pdfs_finep'
os.makedirs(pdf_dir, exist_ok=True)

# Lista para armazenar os textos extraídos dos PDFs
pdf_texts = []

# Loop sobre os links no dataframe
for index, row in df.iterrows():
    pdf_link = row['link']
    edital_number = row['edital_number']

    # Nome do arquivo PDF a ser salvo
    pdf_filename = f'edital_{edital_number}_{index}.pdf'
    pdf_path = os.path.join(pdf_dir, pdf_filename)

    # Baixar o PDF
    print(f"Baixando {pdf_link}...")
    if download_pdf(pdf_link, pdf_path):
        # Extrair o texto do PDF somente se o download foi bem-sucedido
        print(f"Extraindo texto de {pdf_filename}...")
        pdf_text = extract_text_from_pdf(pdf_path)
    else:
        pdf_text = "Erro no download, não foi possível extrair o texto."

    # Armazenar no dataframe
    pdf_texts.append({'edital_number': edital_number, 'pdf_link': pdf_link, 'pdf_text': pdf_text})

# Converter a lista em um dataframe
df_texts = pd.DataFrame(pdf_texts)

# Salvar o dataframe com o texto dos PDFs
df_texts.to_csv('editais_finep_textos.csv', index=False)

print("Extração de textos concluída e dados salvos em 'editais_finep_textos.csv'")

df_texts

"""### Criação do RAG

#### Embedding
"""

!pip install -q transformers sentence_transformers faiss-cpu torch nltk

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pandas as pd
import os
import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize
from google.colab import userdata

HUGGING_FACE_ACCESS_TOKEN = userdata.get('HUGGING_FACE_ACCESS_TOKEN')

model_name = 'google/gemma-2-2b-it'

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    token=HUGGING_FACE_ACCESS_TOKEN
    ).to('cuda')

tokenizer = AutoTokenizer.from_pretrained(model_name, token=HUGGING_FACE_ACCESS_TOKEN)

def split_text_into_chunks(text, max_chunk_size=1000):
    sentences = sent_tokenize(text)
    chunks = []
    current_chunk = ""

    for sentence in sentences:
        if len(current_chunk) + len(sentence) <= max_chunk_size:
            current_chunk += sentence + " "
        else:
            chunks.append(current_chunk.strip())
            current_chunk = sentence + " "

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks

encoder = SentenceTransformer('all-MiniLM-L6-v2')

def text_to_embeddings(text):
    if isinstance(text, str):
        chunks = split_text_into_chunks(text)
        document_embeddings = encoder.encode(chunks)
        return document_embeddings
    return np.array([])

def get_chunks(text):
  if isinstance(text, str):
    chunks = split_text_into_chunks(text)
    return chunks
  return []

df_texts = pd.DataFrame()
try:
  df_texts = pd.read_csv('editais_finep_textos.csv')
except:
  df_texts = df_texts
  print("Arquivo não encontrado, utilizando dataframe em memória")
df_rag = df_texts.copy()
df_rag['embeddings'] = df_rag['pdf_text'].apply(text_to_embeddings)
df_rag['text_chunks'] = df_rag['pdf_text'].apply(get_chunks)

"""#### Vector Store"""

df_rag

# all_embeddings = np.vstack(df_rag['embeddings'].tolist())
valid_embeddings = df_rag['embeddings'].apply(lambda x: len(x) > 0)
filtered_embeddings = df_rag[valid_embeddings]['embeddings'].tolist()
all_embeddings = np.vstack(filtered_embeddings)
dimension = all_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(all_embeddings)
faiss.write_index(index, "editais_faiss_index.bin")

"""#### Funções de busca de similaridade"""

index = faiss.read_index("editais_faiss_index.bin")
print("Índice FAISS carregado com sucesso!")

def find_most_similar_chunks(query, top_k=1000):
    query_embedding = encoder.encode([query])
    distances, indices = index.search(query_embedding, top_k)
    results = []
    # total_chunks = sum(len(chunks) for chunks in df_rag['text_chunks'])
    total_chunks = sum(len(chunks) if chunks is not None else 0 for chunks in df_rag['text_chunks'])
    for i, idx in enumerate(indices[0]):
        if idx < total_chunks:
            doc_idx = 0
            chunk_idx = idx
            while chunk_idx >= len(df_rag['text_chunks'].iloc[doc_idx]):
                chunk_idx -= len(df_rag['text_chunks'].iloc[doc_idx])
                doc_idx += 1
            results.append({
                'edital': df_rag['edital_number'].iloc[doc_idx],
                'document': df_rag['pdf_link'].iloc[doc_idx],
                'chunk': df_rag['text_chunks'].iloc[doc_idx][chunk_idx],
                'distance': distances[0][i]
            })
    return results

def query_documents(query):
    similar_chunks = find_most_similar_chunks(query)
    context = " ".join([result['chunk'].replace("\n", "") for result in similar_chunks])
    # response = generate_response(query, context)
    return similar_chunks

most_similar = find_most_similar_chunks("O meu projeto é sobre Inteligência artificial, matchmaking de projetos com editais de fomento à inovação", top_k=100)
most_similar

"""TODO: Pensar uma maneira de ordenar dependendo de quantas vezes aparecem, dando um peso pela distância"""

from collections import defaultdict
def aggregate_similarities_by_edital(similar_chunks):
    edital_similarity = defaultdict(list)

    # Agrupar chunks por edital
    for result in similar_chunks:
        edital_similarity[result['edital']].append(result['distance'])

    # Calcular a média das distâncias para cada edital
    edital_avg_similarity = {}
    for edital, distances in edital_similarity.items():
        avg_distance = sum(distances) / len(distances)
        edital_avg_similarity[edital] = avg_distance

    # Ordenar os editais pela menor média de distância (maior similaridade)
    sorted_edital_similarity = sorted(edital_avg_similarity.items(), key=lambda x: x[1])

    return sorted_edital_similarity

aggregate_similarities_by_edital(most_similar)

most_similar = find_most_similar_chunks("O meu projeto é sobre tratamento de água com menos poluentes", top_k=100)
aggregate_similarities_by_edital(most_similar)

"""## Nome do edital"""

import requests
from lxml import html
import pandas as pd

# Função para extrair o conteúdo com base no XPath
def importxml(page, xpath):
    tree = html.fromstring(page.content)
    content_list = tree.xpath(xpath)
    content = content_list[0].text_content() if len(content_list) > 0 else ''
    return content.strip()

# Função para extrair todos os links de PDF de um edital
def extract_name_from_edital(url):
    page = requests.get(url)

    name = html.fromstring(page.content).xpath('//*[@id="componente"]/div/h2/a')

    if len(name):
      print(name[0].text)
      name = name[0].text
    else:
      name = ''

    return name

# Lista para armazenar as informações
data = []

editais_numbers = list(range(586, 748))

# Loop por cada edital
for edital_number in editais_numbers:
    url = f'http://finep.gov.br/chamadas-publicas/chamadapublica/{edital_number}'
    name = extract_name_from_edital(url)

    # Adiciona cada PDF do edital à lista de dados

    data.append({'edital_number': edital_number, 'edital_name': name})

# Convertendo a lista em dataframe
df_names = pd.DataFrame(data)

# Salvar os dados em um arquivo CSV
# df.to_csv('editais_finep.csv', index=False)

print("Extração concluída e dados salvos em 'editais_finep.csv'")
df_names

df_names.to_csv('editais_finep_names.csv', index=False)
df_names.to_json('editais_finep_names.json', orient='records')